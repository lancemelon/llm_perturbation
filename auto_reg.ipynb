{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a266e6",
   "metadata": {},
   "source": [
    "# Autoregressive Generation Wavelet Perturbation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b8c9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pywt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b92f1be",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71aac30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\" \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba4765",
   "metadata": {},
   "source": [
    "### Tokenize Input Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa6f8080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[45001,   318,  1900,   329,   644,  2057,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Italy is known for what food?\"\n",
    "inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997f9ae",
   "metadata": {},
   "source": [
    "### Extract Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f109145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    embeddings = model.transformer.wte(input_ids)  # shape: [1, seq_len, embed_dim]\n",
    "\n",
    "embeddings[0].shape  # (seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591b426",
   "metadata": {},
   "source": [
    "### Wavelet Perturbation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d842b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_perturb(embeddings_tensor, wavelet='haar'):\n",
    "    \"\"\"\n",
    "    Applies discrete wavelet transform per token embedding vector,\n",
    "    zeros out high-frequency coefficients, and reconstructs embeddings.\n",
    "    \"\"\"\n",
    "    emb_np = embeddings_tensor.squeeze(0).cpu().numpy()  # [seq_len, embed_dim]\n",
    "    perturbed_np = np.zeros_like(emb_np)\n",
    "\n",
    "    for i in range(emb_np.shape[0]):  # for each token vector\n",
    "        coeffs = pywt.dwt(emb_np[i, :], wavelet)\n",
    "        cA, cD = coeffs\n",
    "        # Zero out high frequency coefficients\n",
    "        cD[:] = 0\n",
    "        # Reconstruct embedding\n",
    "        perturbed_np[i, :] = pywt.idwt(cA, cD, wavelet)\n",
    "\n",
    "    perturbed_tensor = torch.tensor(perturbed_np, dtype=torch.float32).unsqueeze(0)\n",
    "    return perturbed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80abcb",
   "metadata": {},
   "source": [
    "### Apply Wavelet Perturbation to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a53e5a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_embeddings = wavelet_perturb(embeddings)\n",
    "perturbed_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a953f",
   "metadata": {},
   "source": [
    "### Test Model Inputs with Original and Perturbed Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5985fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_k=50):\n",
    "    # Temperature scaling\n",
    "    logits = logits / temperature\n",
    "    # Top-k filtering\n",
    "    top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "    probs = torch.softmax(top_k_values, dim=-1)\n",
    "    next_token = top_k_indices[0, torch.multinomial(probs, num_samples=1)]\n",
    "    return next_token.unsqueeze(0)  # [1, 1]\n",
    "\n",
    "def generate_from_embeddings(start_embeddings, start_ids, max_new_tokens=20):\n",
    "    generated_embeds = start_embeddings\n",
    "    generated_ids = start_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs_embeds=generated_embeds)\n",
    "            next_token_logits = outputs.logits[:, -1, :]  # [1, vocab_size]\n",
    "            next_token_id = sample_next_token(next_token_logits)  # [1, 1]\n",
    "\n",
    "        # Ensure next_token_id is 2D\n",
    "        next_token_id = next_token_id.view(1, 1)\n",
    "\n",
    "        # Append new token ID\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        # Append new token embedding\n",
    "        next_token_emb = model.transformer.wte(next_token_id)  # [1, 1, embed_dim]\n",
    "        generated_embeds = torch.cat([generated_embeds, next_token_emb], dim=1)\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01491c",
   "metadata": {},
   "source": [
    "### Decode Output Logits\n",
    "\n",
    "Prompt: \"Italy is known for what food?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d0ece1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original generation:\n",
      "Italy is known for what food? But what are they? If there was a restaurant near the coast to where I live on the beach, it would be on my way home for a vacation.\n",
      "\n",
      "There's an idea that a restaurant is the answer to being in a different world\n",
      "\n",
      "Perturbed generation:\n",
      "Italy is known for what food?\n",
      "\n",
      "The following are my thoughts on this issue. I would like to remind the people of my country, that I, and the government my country, have been a part of this and I would like to be an observer in this, and one\n"
     ]
    }
   ],
   "source": [
    "original_text = generate_from_embeddings(embeddings, input_ids, max_new_tokens=50)\n",
    "perturbed_text = generate_from_embeddings(perturbed_embeddings, input_ids, max_new_tokens=50)\n",
    "\n",
    "print(\"Original generation:\")\n",
    "print(original_text)\n",
    "print(\"\\nPerturbed generation:\")\n",
    "print(perturbed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c04bef",
   "metadata": {},
   "source": [
    "### Quantify Changes in Output Logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Perturbation",
   "language": "python",
   "name": "pert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

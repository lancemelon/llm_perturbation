{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61819bf",
   "metadata": {},
   "source": [
    "## Load packages and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d869258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d6844d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\" \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f3445",
   "metadata": {},
   "source": [
    "## Load and tokenize prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c579eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Who discovered penicillin?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>When was the Eiffel Tower built?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the capital of Australia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How tall is Mount Everest?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Who painted the Mona Lisa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>Finish: 'In the morning, I always…'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Give an example of a metaphor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>List two programming languages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Write a haiku about winter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Name three famous inventors.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                               prompt\n",
       "0    1           Who discovered penicillin?\n",
       "1    2     When was the Eiffel Tower built?\n",
       "2    3    What is the capital of Australia?\n",
       "3    4           How tall is Mount Everest?\n",
       "4    5           Who painted the Mona Lisa?\n",
       "..  ..                                  ...\n",
       "94  95  Finish: 'In the morning, I always…'\n",
       "95  96       Give an example of a metaphor.\n",
       "96  97      List two programming languages.\n",
       "97  98          Write a haiku about winter.\n",
       "98  99         Name three famous inventors.\n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/prompts.csv\", quotechar='\"')\n",
    "df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9540d72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 8241,  5071,  3112,   291, 32672,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompts = []\n",
    "\n",
    "for prompt in df['prompt']:\n",
    "    tokenized_prompts.append(tokenizer.encode_plus(prompt, return_tensors=\"pt\"))\n",
    "\n",
    "tokenized_prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e43812",
   "metadata": {},
   "source": [
    "## Define Pertrubing methods\n",
    "\n",
    "1. Zeroing High-Frequency\n",
    "2. Zeroing Low-Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "358a4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_perturb(embeddings_tensor, wavelet='haar'):\n",
    "    \"\"\"\n",
    "    Applies discrete wavelet transform per token embedding vector,\n",
    "    zeros out high-frequency coefficients, and reconstructs embeddings.\n",
    "    \"\"\"\n",
    "    emb_np = embeddings_tensor.squeeze(0).cpu().numpy()  # [seq_len, embed_dim]\n",
    "    perturbed_np = np.zeros_like(emb_np)\n",
    "\n",
    "    for i in range(emb_np.shape[0]):  # for each token vector\n",
    "        coeffs = pywt.dwt(emb_np[i, :], wavelet)\n",
    "        cA, cD = coeffs\n",
    "        # Zero out high frequency coefficients\n",
    "        cD[:] = 0\n",
    "        # Reconstruct embedding\n",
    "        perturbed_np[i, :] = pywt.idwt(cA, cD, wavelet)\n",
    "\n",
    "    perturbed_tensor = torch.tensor(perturbed_np, dtype=torch.float32).unsqueeze(0)\n",
    "    return perturbed_tensor\n",
    "\n",
    "def wavelet_perturb2(embeddings_tensor, wavelet='haar'):\n",
    "    \"\"\"\n",
    "    Applies discrete wavelet transform per token embedding vector,\n",
    "    zeros out low-frequency coefficients, and reconstructs embeddings.\n",
    "    \"\"\"\n",
    "    emb_np = embeddings_tensor.squeeze(0).cpu().numpy()  # [seq_len, embed_dim]\n",
    "    perturbed_np = np.zeros_like(emb_np)\n",
    "\n",
    "    for i in range(emb_np.shape[0]):  # for each token vector\n",
    "        coeffs = pywt.dwt(emb_np[i, :], wavelet)\n",
    "        cA, cD = coeffs\n",
    "        # Zero out high frequency coefficients\n",
    "        cA[:] = 0\n",
    "        # Reconstruct embedding\n",
    "        perturbed_np[i, :] = pywt.idwt(cA, cD, wavelet)\n",
    "\n",
    "    perturbed_tensor = torch.tensor(perturbed_np, dtype=torch.float32).unsqueeze(0)\n",
    "    return perturbed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710844d8",
   "metadata": {},
   "source": [
    "## Embed tokenized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab685e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in tokenized_prompts:\n",
    "        input_ids = prompt['input_ids']\n",
    "        embedding = model.transformer.wte(input_ids)\n",
    "        embeddings.append(embedding.cpu().numpy())\n",
    "\n",
    "embeddings[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdb4df",
   "metadata": {},
   "source": [
    "## Define pertrubed and original inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34520bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 768])\n",
      "torch.Size([1, 9, 768])\n",
      "torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "# Zero out high frequency components\n",
    "zero_high_freq = []\n",
    "\n",
    "for emb in embeddings:\n",
    "    emb_tensor = torch.tensor(emb, dtype=torch.float32)\n",
    "    perturbed_emb = wavelet_perturb(emb_tensor)\n",
    "    zero_high_freq.append(perturbed_emb)\n",
    "\n",
    "# Zero out low frequency components\n",
    "zero_low_freq = []\n",
    "\n",
    "for emb in embeddings:\n",
    "    emb_tensor = torch.tensor(emb, dtype=torch.float32)\n",
    "    perturbed_emb = wavelet_perturb2(emb_tensor)\n",
    "    zero_low_freq.append(perturbed_emb)\n",
    "\n",
    "# Original embeddings\n",
    "original_embeddings = []\n",
    "\n",
    "for emb in embeddings:\n",
    "    original_embeddings.append(torch.tensor(emb, dtype=torch.float32))\n",
    "\n",
    "print(zero_high_freq[1].shape)\n",
    "print(zero_low_freq[1].shape)\n",
    "print(original_embeddings[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bf022",
   "metadata": {},
   "source": [
    "## Auto-regressive top-k logits\n",
    "\n",
    "- (x = 5) x auto-regressive tokens to predict\n",
    "- (k = 10) top-k logits for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "633129ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_topk(model, tokenizer, embeddings_list, x=5, k=10):\n",
    "    \"\"\"\n",
    "    embeddings_list: list of torch tensors, each (1, seq_len, hidden_dim)\n",
    "    Returns: list of length len(embeddings_list), each element is a list of x steps,\n",
    "             each step is a list of k top-k token strings\n",
    "    \"\"\"\n",
    "    all_topk_predictions = []\n",
    "\n",
    "    for emb in embeddings_list:\n",
    "        topk_predictions = []\n",
    "        generated_embeds = emb.clone()  # (1, seq_len, hidden_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(x):\n",
    "                outputs = model(inputs_embeds=generated_embeds)  # (1, seq_len, vocab_size)\n",
    "                next_token_logits = outputs.logits[:, -1, :]     # (1, vocab_size)\n",
    "\n",
    "                # Top-k\n",
    "                topk_probs, topk_ids = torch.topk(next_token_logits, k=k, dim=-1)\n",
    "                topk_tokens = [tokenizer.decode(ids).strip() for ids in topk_ids[0]]\n",
    "                topk_predictions.append(topk_tokens)\n",
    "\n",
    "                # Append top-1 token embedding to continue autoregressively\n",
    "                next_token_id = topk_ids[:, 0].unsqueeze(-1)  # (1,1)\n",
    "                next_token_embed = model.transformer.wte(next_token_id)  # (1,1,hidden_dim)\n",
    "                generated_embeds = torch.cat([generated_embeds, next_token_embed], dim=1)\n",
    "\n",
    "        all_topk_predictions.append(topk_predictions)\n",
    "\n",
    "    return all_topk_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304632d9",
   "metadata": {},
   "source": [
    "## Extract top-k logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea53291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c62e8c0c",
   "metadata": {},
   "source": [
    "## Softmax for interpreatible probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9ebc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f25ab086",
   "metadata": {},
   "source": [
    "## Define probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb68d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Perturbation",
   "language": "python",
   "name": "pert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
